{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column widths\n",
    "colspecs = [(2, 34), (36, 37), (37, 42)]\n",
    "\n",
    "# Read the file\n",
    "sc = pd.read_fwf('../datasets/street_dict.txt', colspecs=colspecs, header=None, names=[\"address_name\", \"borough\", \"street_code\"], dtype='string')\n",
    "\n",
    "# Mappin based on documentation from here: https://data.cityofnewyork.us/City-Government/Street-Name-Dictionary/w4v2-rv6b/about_data\n",
    "sc.loc[sc.borough == \"1\", \"borough\"] = 'NY'\n",
    "sc.loc[sc.borough == \"2\", \"borough\"] = 'BX'\n",
    "sc.loc[sc.borough == \"3\", \"borough\"] = 'K'\n",
    "sc.loc[sc.borough == \"4\", \"borough\"] = 'Q'\n",
    "sc.loc[sc.borough == \"5\", \"borough\"] = 'R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: ../datasets/hs/2021.csv \n",
      " New: 421 \n",
      " Old: 442\n",
      "------------------------\n",
      "Year: ../datasets/hs/2016.csv \n",
      " New: 412 \n",
      " Old: 437\n",
      "------------------------\n",
      "Year: ../datasets/hs/2017.csv \n",
      " New: 416 \n",
      " Old: 440\n",
      "------------------------\n",
      "Year: ../datasets/hs/2018.csv \n",
      " New: 414 \n",
      " Old: 435\n",
      "------------------------\n",
      "Year: ../datasets/hs/2019.csv \n",
      " New: 408 \n",
      " Old: 427\n",
      "------------------------\n",
      "Year: ../datasets/hs/2020.csv \n",
      " New: 408 \n",
      " Old: 427\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "base_path = '../datasets/hs/'\n",
    "years = ['2021', '2016', '2017', '2018', '2019', '2020']\n",
    "files = [file for year in years for file in glob.glob(base_path + year + \".csv\")]\n",
    "\n",
    "def fuzzy_match(row, threshold=97):  # increase the threshold for a near-perfect match\n",
    "    return fuzz.token_sort_ratio(row['primary_address_line_1'], row['address_name']) > threshold\n",
    "\n",
    "for file in files:\n",
    "    hs = pd.read_csv(file) \n",
    "\n",
    "    cols_of_use = ['boro', 'school_name', 'primary_address_line_1', 'DataYear']\n",
    "    hs = hs[cols_of_use]\n",
    "\n",
    "\n",
    "    hs.loc[hs.boro == \"M\", \"boro\"] = 'NY'\n",
    "    hs.loc[hs.boro == \"X\", \"boro\"] = 'BX'\n",
    "\n",
    "    hs.loc[hs.boro == \"Manhattan\", \"boro\"] = 'NY'\n",
    "    hs.loc[hs.boro == \"Bronx\", \"boro\"] = 'BX'\n",
    "    hs.loc[hs.boro == \"Queens\", \"boro\"] = 'Q'\n",
    "    hs.loc[hs.boro == \"Staten Island\", \"boro\"] = 'R'\n",
    "    hs.loc[hs.boro == \"Brooklyn\", \"boro\"] = 'K'\n",
    "\n",
    "    # Convert the columns to lowercase to make the matching easier\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.lower()\n",
    "    sc['address_name'] = sc['address_name'].str.lower()\n",
    "\n",
    "    # Remove extra whitespaces between words in 'primary_address_line_1' and 'address_name'\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('\\s+', ' ', regex=True)\n",
    "    sc['address_name'] = sc['address_name'].str.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    # Remove suffixes like 'th', 'st', 'nd', 'rd' from 'primary_address_line_1'\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('(\\d+)(st|nd|rd|th)', r'\\1', regex=True)\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('st.', 'st')\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('ave.', 'avenue')\n",
    "    \n",
    "    # First, join on borough column\n",
    "    merged = sc.merge(hs, left_on='borough', right_on='boro', suffixes=('', '_y'), how='inner')\n",
    "    merged = merged.drop(columns='boro')\n",
    "\n",
    "    merged['primary_address_line_1'] = merged['primary_address_line_1'].str.replace('^\\S+\\s*', '', regex=True)\n",
    "\n",
    "    # Fuzzy match entry addreses. Not needed anymore. I used it to see what are the difference in the street so I can\n",
    "    # add the rules above\n",
    "    mask = merged.apply(fuzzy_match, axis=1)\n",
    "\n",
    "    temp = merged[mask]\n",
    "    diff = temp[temp.address_name != temp.primary_address_line_1]\n",
    "    print(f\"Year: {file} \\n New: {len(temp)} \\n Old: {len(hs)}\")\n",
    "    print(\"------------------------\")\n",
    "    merged = merged.drop(columns='primary_address_line_1')\n",
    "\n",
    "    if '2021' in file:\n",
    "        merged[mask].to_csv('../datasets/hs/combined_hs_with_street_codes.csv', mode='w', header=True, index=False)\n",
    "\n",
    "    merged[mask].to_csv('../datasets/hs/combined_hs_with_street_codes.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the file a parquet since append is not supported for parquet file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hssc = pd.read_csv(\"../datasets/hs/combined_hs_with_street_codes.csv\", low_memory=False)\n",
    "hssc.to_parquet(\"../datasets/hs/combined_hs_with_street_codes.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hspk = pd.read_parquet(\"../datasets/hs/combined_hs_with_street_codes.parquet\")\n",
    "hspk = hspk.groupby(['street_code', 'borough', 'address_name', 'DataYear']).size().reset_index(name='school_count')\n",
    "hspk['street_code'] = hspk['street_code'].astype('string')\n",
    "\n",
    "\n",
    "hspk = sc.merge(hspk, left_on=['street_code', 'borough'], right_on=['street_code','borough'], suffixes=('', '_y'), how=\"left\")\n",
    "hspk.drop(columns=['address_name_y'], inplace=True)\n",
    "hspk.fillna({'school_count': 0}, inplace=True)\n",
    "hspk['school_count'] = hspk['school_count'].astype(int)\n",
    "\n",
    "\n",
    "hspk.to_parquet(\"../datasets/hs/hs_per_street.parquet\", compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
