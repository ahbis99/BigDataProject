{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column widths\n",
    "colspecs = [(2, 34), (35, 36), (36, 37), (37, 42)]\n",
    "\n",
    "# Read the file\n",
    "sc = pd.read_fwf('../datasets/street_dict.txt', colspecs=colspecs, header=None, names=[\"address_name\", \"f\", \"borough\", \"street_code\"], dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappin based on documentation from here: https://data.cityofnewyork.us/City-Government/Street-Name-Dictionary/w4v2-rv6b/about_data\n",
    "sc.loc[sc.borough == \"1\", \"borough\"] = 'NY'\n",
    "sc.loc[sc.borough == \"2\", \"borough\"] = 'BX'\n",
    "sc.loc[sc.borough == \"3\", \"borough\"] = 'K'\n",
    "sc.loc[sc.borough == \"4\", \"borough\"] = 'Q'\n",
    "sc.loc[sc.borough == \"5\", \"borough\"] = 'R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address_name</th>\n",
       "      <th>f</th>\n",
       "      <th>borough</th>\n",
       "      <th>street_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>MADISON AVENUE</td>\n",
       "      <td>F</td>\n",
       "      <td>NY</td>\n",
       "      <td>25390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12088</th>\n",
       "      <td>191 STREET</td>\n",
       "      <td>F</td>\n",
       "      <td>Q</td>\n",
       "      <td>25390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17716</th>\n",
       "      <td>CURTIS AVENUE</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>25390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         address_name  f borough street_code\n",
       "1963   MADISON AVENUE  F      NY       25390\n",
       "12088      191 STREET  F       Q       25390\n",
       "17716   CURTIS AVENUE  F       R       25390"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc[sc.street_code == '25390'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: ../datasets/hs/2021.csv \n",
      " New: 421 \n",
      " Old: 442\n",
      "------------------------\n",
      "Year: ../datasets/hs/2016.csv \n",
      " New: 412 \n",
      " Old: 437\n",
      "------------------------\n",
      "Year: ../datasets/hs/2017.csv \n",
      " New: 416 \n",
      " Old: 440\n",
      "------------------------\n",
      "Year: ../datasets/hs/2018.csv \n",
      " New: 414 \n",
      " Old: 435\n",
      "------------------------\n",
      "Year: ../datasets/hs/2019.csv \n",
      " New: 408 \n",
      " Old: 427\n",
      "------------------------\n",
      "Year: ../datasets/hs/2020.csv \n",
      " New: 408 \n",
      " Old: 427\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "base_path = '../datasets/hs/'\n",
    "years = ['2021', '2016', '2017', '2018', '2019', '2020']\n",
    "files = [file for year in years for file in glob.glob(base_path + year + \".csv\")]\n",
    "\n",
    "def fuzzy_match(row, threshold=97):  # increase the threshold for a near-perfect match\n",
    "    return fuzz.token_sort_ratio(row['primary_address_line_1'], row['address_name']) > threshold\n",
    "\n",
    "for file in files:\n",
    "    hs = pd.read_csv(file) \n",
    "\n",
    "    hs = hs.drop(columns=hs.filter(regex='prgdesc\\d*').columns)\n",
    "    hs = hs.drop(columns=hs.filter(regex='requirement\\d*').columns)\n",
    "    hs = hs.drop(columns=hs.filter(regex='eligibility\\d*').columns)\n",
    "    hs = hs.drop(columns=hs.filter(regex='auditioninformation\\d*').columns)\n",
    "    hs = hs.drop(columns=hs.filter(regex='admissionspriority\\d*').columns)\n",
    "\n",
    "\n",
    "    hs.loc[hs.boro == \"M\", \"boro\"] = 'NY'\n",
    "    hs.loc[hs.boro == \"X\", \"boro\"] = 'BX'\n",
    "\n",
    "    hs.loc[hs.boro == \"Manhattan\", \"boro\"] = 'NY'\n",
    "    hs.loc[hs.boro == \"Bronx\", \"boro\"] = 'BX'\n",
    "    hs.loc[hs.boro == \"Queens\", \"boro\"] = 'Q'\n",
    "    hs.loc[hs.boro == \"Staten Island\", \"boro\"] = 'R'\n",
    "    hs.loc[hs.boro == \"Brooklyn\", \"boro\"] = 'K'\n",
    "\n",
    "    # Convert the columns to lowercase to make the matching easier\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.lower()\n",
    "    sc['address_name'] = sc['address_name'].str.lower()\n",
    "\n",
    "    # Remove extra whitespaces between words in 'primary_address_line_1' and 'address_name'\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('\\s+', ' ', regex=True)\n",
    "    sc['address_name'] = sc['address_name'].str.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    # Remove suffixes like 'th', 'st', 'nd', 'rd' from 'primary_address_line_1'\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('(\\d+)(st|nd|rd|th)', r'\\1', regex=True)\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('st.', 'st')\n",
    "    hs['primary_address_line_1'] = hs['primary_address_line_1'].str.replace('ave.', 'avenue')\n",
    "    \n",
    "    # First, join on borough column\n",
    "    merged = sc.merge(hs, left_on='borough', right_on='boro', suffixes=('', '_y'), how='inner')\n",
    "\n",
    "    merged['primary_address_line_1'] = merged['primary_address_line_1'].str.replace('^\\S+\\s*', '', regex=True)\n",
    "\n",
    "    # Fuzzy match entry addreses. Not needed anymore. I used it to see what are the difference in the street so I can\n",
    "    # add the rules above\n",
    "    mask = merged.apply(fuzzy_match, axis=1)\n",
    "\n",
    "    temp = merged[mask]\n",
    "    diff = temp[temp.address_name != temp.primary_address_line_1]\n",
    "    print(f\"Year: {file} \\n New: {len(temp)} \\n Old: {len(hs)}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    missing_schools = hs[~hs['school_name'].isin(temp['school_name'])]\n",
    "\n",
    "    cols_to_save = ['street_code', 'address_name', 'borough', 'DataYear', 'school_name']\n",
    "    if '2021' in file:\n",
    "        merged[mask][cols_to_save].to_csv('../datasets/hs/combined_hs_with_street_codes.csv', mode='w', header=True, index=False)\n",
    "\n",
    "    merged[mask][cols_to_save].to_csv('../datasets/hs/combined_hs_with_street_codes.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the file a parquet since append is not supported for parquet file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hssc = pd.read_csv(\"../datasets/hs/combined_hs_with_street_codes.csv\", low_memory=False)\n",
    "hssc.to_parquet(\"../datasets/hs/combined_hs_with_street_codes.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hspk = pd.read_parquet(\"../datasets/hs/combined_hs_with_street_codes.parquet\")\n",
    "hspk = hspk.groupby(['street_code', 'borough', 'address_name', 'DataYear']).size().reset_index(name='school_count')\n",
    "hspk['street_code'] = hspk['street_code'].astype('string')\n",
    "\n",
    "\n",
    "hspk = sc.merge(hspk, left_on=['street_code', 'borough'], right_on=['street_code','borough'], suffixes=('', '_y'), how=\"left\")\n",
    "hspk.drop(columns=['address_name_y'], inplace=True)\n",
    "hspk.fillna({'school_count': 0}, inplace=True)\n",
    "hspk['school_count'] = hspk['school_count'].astype(int)\n",
    "\n",
    "\n",
    "hspk.to_parquet(\"../datasets/hs/hs_per_street.parquet\", compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
