{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Perform the data analysis in a “streaming” manner (treat data as streaming). \n",
    "\n",
    "Use Kafka as a message broker and write a custom producer (reading data from files) and a custom consumer\n",
    "(for processing data). \n",
    "\n",
    "Since data is (with few exceptions, most likely errors) ordered by the “summons number” you can assume that the lines in CSV files are in chronological order\n",
    "(important for producing Kafka messages). \n",
    "\n",
    "Decide if using different topics can help you. \n",
    "\n",
    "Show rolling descriptive statistics (mean, standard deviation, …, think of at least three more) for all data, boroughs, and for the 10 most interesting streets (with highest numbers of tickets overall, or by your qualified choice). \n",
    "\n",
    "For the same data, choose, implement, and apply a stream clustering algorithm (preferably spatial clustering) of your choice [7].\n",
    "\n",
    "Note: stream processing is better not performed in the Arnes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, uuid, json\n",
    "import pandas as pd\n",
    "import faust, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../datasets/original_data/'\n",
    "\n",
    "csv_files = [f for f in os.listdir(base_path) if f.endswith('.csv')]\n",
    "csv_files.sort()\n",
    "\n",
    "street_locations = pd.read_csv(base_path + 'street_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk):\n",
    "    chunk = chunk.dropna(subset=['summons_number'])\n",
    "    chunk = chunk.drop_duplicates()\n",
    "\n",
    "    chunk['street_code'] = chunk['street_code1'].where(chunk['street_code1'] != 0, chunk['street_code2'].where(chunk['street_code2'] != 0, chunk['street_code3'])).astype(\"string\")\n",
    "    chunk['street_code'] = chunk['street_code'].replace('0', pd.NA)\n",
    "    chunk.drop(['street_code1', 'street_code2', 'street_code3'], axis=1, inplace=True)\n",
    "    \n",
    "    chunk['issue_date'] = pd.to_datetime(chunk['issue_date'], format=\"mixed\")\n",
    "    \n",
    "    chunk['violation_time'] = chunk['violation_time'].astype(str)\n",
    "    # add M to the end of the time to read it from 12 hour format\n",
    "    chunk['violation_time'] = chunk[\"violation_time\"].str.upper() + \"M\"\n",
    "    # replace the values that starts with 00 to 12\n",
    "    chunk['violation_time'] = chunk['violation_time'].str.replace(r'^00', '12', regex=True)\n",
    "    # convert the time to 24 hour format\n",
    "    chunk['violation_time'] = pd.to_datetime(chunk[\"violation_time\"], format=\"%I%M%p\", errors=\"coerce\")\n",
    "    # combine the date and time\n",
    "    chunk['issue_date'] = chunk[\"issue_date\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"violation_time\"].dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    chunk = chunk.dropna(subset=['issue_date'])\n",
    "    chunk = chunk.drop([\"violation_time\"], axis=1)\n",
    "\n",
    "    #chunk['vehicle_expiration_date'] = pd.to_datetime(chunk['vehicle_expiration_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].astype(str)\n",
    "    chunk['time_first_observed'] = chunk[\"time_first_observed\"].str.upper() + \"M\"\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].str.replace(r'^00', '12', regex=True)\n",
    "    chunk['time_first_observed'] = pd.to_datetime(chunk['time_first_observed'], format='%I%M%p', errors='coerce')\n",
    "\n",
    "    # replace 0 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0', pd.NaT)\n",
    "    # replace 0001-01-03T12:00:00.000 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0001-01-03T12:00:00.000', pd.NaT)\n",
    "\n",
    "    chunk['date_first_observed'] = pd.to_datetime(chunk['date_first_observed'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # merge the date and time\n",
    "    chunk['date_first_observed'] = chunk[\"date_first_observed\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"time_first_observed\"].dt.strftime('%H:%M:%S')\n",
    "    chunk = chunk.drop([\"time_first_observed\"], axis=1)\n",
    "\n",
    "    # translate the county names to the borough names\n",
    "    county_to_borough = {\n",
    "        \"BRONX\": \"BX\", # Bronx\n",
    "        \"BX\": \"BX\",\n",
    "        \"Bronx\": \"BX\",\n",
    "        \"BRONX\": \"BX\",\n",
    "        \"BK\": \"K\", # Brooklyn known as Kings\n",
    "        \"K\": \"K\",\n",
    "        \"Kings\": \"K\",\n",
    "        \"KINGS\": \"K\",\n",
    "        \"KING\": \"K\",\n",
    "        \"Q\": \"Q\", # Queens\n",
    "        \"QN\": \"Q\",\n",
    "        \"Qns\": \"Q\",\n",
    "        \"QUEEN\": \"Q\",\n",
    "        \"QUEENS\": \"Q\",\n",
    "        \"QNS\": \"Q\",\n",
    "        \"QU\": \"Q\",\n",
    "        \"NY\": \"NY\", # Manhattan known as New York\n",
    "        \"MN\": \"NY\",\n",
    "        \"MAN\": \"NY\",\n",
    "        \"NEW Y\": \"NY\",\n",
    "        \"NEWY\": \"NY\",\n",
    "        \"NYC\": \"NY\",\n",
    "        \"ST\": \"R\", # Staten Island known as Richmond\n",
    "        \"R\": \"R\",\n",
    "        \"Rich\": \"R\",\n",
    "        \"RICH\": \"R\",\n",
    "        \"RICHM\": \"R\",\n",
    "        \"RC\": \"R\",\n",
    "        \"MH\": \"NY\",\n",
    "        \"MS\": \"NY\",\n",
    "        \"N\": \"NY\",\n",
    "        \"P\": \"NY\",\n",
    "        \"PBX\": \"NY\",\n",
    "        \"USA\": \"NY\",\n",
    "        \"VINIS\": \"NY\",\n",
    "        \"A\": pd.NA,\n",
    "        \"F\": pd.NA,\n",
    "        \"ABX\": pd.NA,\n",
    "        \"108\": pd.NA,\n",
    "        \"103\": \"R\", # Staten Island zip code\n",
    "        \"00000\": pd.NA,\n",
    "        \"K   F\": pd.NA,\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(county_to_borough)\n",
    "\n",
    "    borough_to_code = {\n",
    "    'NY': 1,\n",
    "    'BX': 2,\n",
    "    'K': 3,\n",
    "    'Q': 4,\n",
    "    'R': 5\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(borough_to_code)\n",
    "\n",
    "    # drop the rows that have NaN in the violation_county\n",
    "    chunk = chunk.dropna(subset=['violation_county'])\n",
    "    \n",
    "    # update street names with real ones\n",
    "    chunk['street_name'] = street_locations.loc[street_locations['street_code'].isin(chunk['street_code']), 'street_name'].values\n",
    "    \n",
    "    chunk['latitude'] = street_locations.loc[street_locations['street_code'].isin(chunk['street_code']), 'latitude'].values\n",
    "    chunk['longitude'] = street_locations.loc[street_locations['street_code'].isin(chunk['street_code']), 'longitude'].values\n",
    "\n",
    "    chunk = chunk.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1723198788.786|TERMINATE|cristian-pc#producer-13| [thrd:app]: Producer terminating with 8 messages (8689 bytes) still in queue or transit: use flush() to wait for outstanding message delivery\n"
     ]
    }
   ],
   "source": [
    "producer = kafka.Producer({'bootstrap.servers': \"localhost:29092\",\n",
    "                  'client.id': socket.gethostname()})\n",
    "                  \n",
    "consumer = kafka.Consumer({'bootstrap.servers': \"localhost:29092\",\n",
    "                           'client.id': socket.gethostname(),\n",
    "                           'group.id': 'test_group', \n",
    "                           'auto.offset.reset': 'earliest'})\n",
    "\n",
    "topic = \"parking_violations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parking_violations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parking_violations.py\n",
    "\n",
    "import faust\n",
    "\n",
    "class ParkingViolation(faust.Record, validation=True):\n",
    "    summons_number: int\n",
    "    plate_id: str\n",
    "    registration_state: str\n",
    "    plate_type: str\n",
    "    violation_code: str\n",
    "    vehicle_body_type: str\n",
    "    vehicle_make: str\n",
    "    issuing_agency: str\n",
    "    street_code: str\n",
    "    vehicle_expiration_date: str\n",
    "    violation_location: str\n",
    "    violation_precinct: str\n",
    "    issuer_precinct: str\n",
    "    issuer_code: str\n",
    "    issuer_command: str\n",
    "    issuer_squad: str\n",
    "    violation_county: str\n",
    "    violation_in_front_of_or_opposite: str\n",
    "    house_number: str\n",
    "    street_name: str\n",
    "    intersecting_street: str\n",
    "    date_first_observed: str\n",
    "    law_section: str\n",
    "    sub_division: str\n",
    "    violation_legal_code: str\n",
    "    days_parking_in_effect: str\n",
    "    from_hours_in_effect: str\n",
    "    to_hours_in_effect: str\n",
    "    vehicle_color: str\n",
    "    unregistered_vehicle: str\n",
    "    vehicle_year: str\n",
    "    meter_number: str\n",
    "    feet_from_curb: str\n",
    "    violation_post_code: str\n",
    "    violation_description: str\n",
    "    no_standing_or_stopping_violation: str\n",
    "    hydrant_violation: str\n",
    "    double_parking_violation: str\n",
    "    latitude: str\n",
    "    longitude: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream_stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_stats.py\n",
    "\n",
    "class StreamStats():\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def update(self, value):\n",
    "        self.count += 1\n",
    "\n",
    "        self.sum += value\n",
    "        self.squared_sum += value ** 2\n",
    "        self.mean = self.sum / self.count\n",
    "\n",
    "        self.min = min(self.min, value)\n",
    "        self.max = max(self.max, value)\n",
    "\n",
    "        self.std = (self.squared_sum / self.count - self.mean ** 2) ** 0.5\n",
    "\n",
    "    def clear(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"mean: {self.mean} +/- {self.std}, min: {self.min}, max: {self.max}\"\n",
    "\n",
    "# Update the stats for each year, month, day, and hour\n",
    "class StreamDateStats():\n",
    "    def __init__(self):\n",
    "        self.year = StreamStats()\n",
    "        self.month = StreamStats()\n",
    "        self.day = StreamStats()\n",
    "\n",
    "        self.year_count = 0\n",
    "        self.month_count = 0\n",
    "        self.day_count = 0\n",
    "\n",
    "        self.total_count = 0\n",
    "\n",
    "    def increase(self):\n",
    "        self.year_count += 1\n",
    "        self.month_count += 1\n",
    "        self.day_count += 1\n",
    "\n",
    "        self.total_count += 1\n",
    "\n",
    "    def reset_year(self):\n",
    "        self.year.update(self.year_count)\n",
    "        self.year_count = 0\n",
    "\n",
    "    def reset_month(self):\n",
    "        self.month.update(self.month_count)\n",
    "        self.month_count = 0\n",
    "\n",
    "    def reset_day(self):\n",
    "        self.day.update(self.day_count)\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_day(self):\n",
    "        self.day.clear()\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_month(self):\n",
    "        self.month.clear()\n",
    "        self.month_count = 0\n",
    "        \n",
    "class CounterDateStats():\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "\n",
    "    def increase(self, key):\n",
    "        if key in [None, \"None\", \"nan\"]:\n",
    "            return\n",
    "\n",
    "        if key not in self.counter:\n",
    "            self.counter[key] = StreamDateStats()\n",
    "\n",
    "        self.counter[key].increase()\n",
    "\n",
    "    def items(self):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)\n",
    "\n",
    "    def most_commons(self, n):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)[:n]\n",
    "\n",
    "    def reset_year(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_year()\n",
    "\n",
    "    def reset_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_month()\n",
    "\n",
    "    def reset_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_day()\n",
    "\n",
    "    def clear_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_month()\n",
    "\n",
    "    def clear_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting faust_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile faust_app.py\n",
    "\n",
    "from typing import List\n",
    "import faust\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "from parking_violations import ParkingViolation\n",
    "from stream_stats import StreamStats, StreamDateStats, CounterDateStats\n",
    "from collections import Counter\n",
    "\n",
    "topic = \"parking_violations\"\n",
    "\n",
    "app = faust.App(topic, broker='kafka://localhost:29092')\n",
    "print(f\"App is {app}\")\n",
    "\n",
    "violations_topic = app.topic(topic, value_type=ParkingViolation)\n",
    "\n",
    "@app.agent(violations_topic)\n",
    "async def process_violations(violations: List[ParkingViolation]):\n",
    "    all_data_stats = StreamDateStats()\n",
    "    borough_stats = CounterDateStats()\n",
    "    streets_stats = CounterDateStats()\n",
    "\n",
    "    tmp_all_data_stats = StreamDateStats()\n",
    "    tmp_borough_stats = CounterDateStats()\n",
    "    tmp_streets_stats = CounterDateStats()\n",
    "\n",
    "    current_year = None\n",
    "    current_month = None\n",
    "    current_day = None\n",
    "\n",
    "    async for violation in violations:\n",
    "        violation.issue_date = datetime.strptime(violation.issue_date, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        if current_year is None:\n",
    "            current_year = violation.issue_date.year\n",
    "            current_month = violation.issue_date.month\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new day starts, reset the day\n",
    "        if violation.issue_date.day != current_day:\n",
    "            tmp_all_data_stats.reset_day()\n",
    "            tmp_borough_stats.reset_day()\n",
    "            tmp_streets_stats.reset_day()\n",
    "\n",
    "            all_data_stats.reset_day()\n",
    "            borough_stats.reset_day()\n",
    "            streets_stats.reset_day()\n",
    "\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new month starts, print the daily stats of the previous month (mean of violations per day) and clear the stats\n",
    "        if violation.issue_date.month != current_month:\n",
    "            tmp_all_data_stats.reset_month()\n",
    "            tmp_borough_stats.reset_month()\n",
    "            tmp_streets_stats.reset_month()\n",
    "\n",
    "            all_data_stats.reset_month()\n",
    "            borough_stats.reset_month()\n",
    "            streets_stats.reset_month()\n",
    "\n",
    "            print(f\"Daily stats of month {current_month} of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                print(f\".      {borough} stats: total: {stats.total_count}, {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_day()\n",
    "            tmp_borough_stats.clear_day()\n",
    "            tmp_streets_stats.clear_day()\n",
    "\n",
    "            current_month = violation.issue_date.month\n",
    "\n",
    "        # if a new year starts, print the stats of the previous year (mean of violations per month) and clear the stats\n",
    "        if violation.issue_date.year != current_year:\n",
    "            tmp_all_data_stats.reset_year()\n",
    "            tmp_borough_stats.reset_year()\n",
    "            tmp_streets_stats.reset_year()\n",
    "\n",
    "            all_data_stats.reset_year()\n",
    "            borough_stats.reset_year()\n",
    "            streets_stats.reset_year()\n",
    "\n",
    "            print(f\"Monthly stats of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.month}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                print(f\".      {borough} stats: total: {stats.total_count}, {stats.month}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].month}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_month()\n",
    "            tmp_borough_stats.clear_month()\n",
    "            tmp_streets_stats.clear_month()\n",
    "\n",
    "            current_year = violation.issue_date.year\n",
    "\n",
    "        # print the overall stats of the data every 2000000 records\n",
    "        if all_data_stats.total_count % 2000000 == 0 and all_data_stats.total_count != 0:\n",
    "            # print overall data stats\n",
    "            print(f\"Overall stats from beginning:\")\n",
    "            print(\"- Overall stats: total: {all_data_stats.total_count}\")\n",
    "            print(f\".      yearly: {all_data_stats.year}\")\n",
    "            print(f\".      monthly: {all_data_stats.month}\")\n",
    "            print(f\".      daily: {all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                print(f\".      {borough} stats: {stats.total_count}\")\n",
    "                print(f\".             yearly: {stats.year}\")\n",
    "                print(f\".             monthly: {stats.month}\")\n",
    "                print(f\".             daily: {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}\")\n",
    "                print(f\".             yearly: {street[1].year}\")\n",
    "                print(f\".             monthly: {street[1].month}\")\n",
    "                print(f\".             daily: {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "        # increase the stats with current violation\n",
    "        all_data_stats.increase()\n",
    "        borough_stats.increase(violation.violation_county)\n",
    "        streets_stats.increase(violation.street_code)\n",
    "\n",
    "        tmp_all_data_stats.increase()\n",
    "        tmp_borough_stats.increase(violation.violation_county)\n",
    "        tmp_streets_stats.increase(violation.street_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in chronological ordered only by issue_date, not by violation_time. So no stats on the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(base_path + csv_files[0], chunksize=10000)\n",
    "    for chunk in df:\n",
    "        chunk = preprocess_chunk(chunk)\n",
    "        for index, line in chunk.iterrows():\n",
    "            year = pd.to_datetime(line['issue_date']).year\n",
    "            if year < 2013 or year > 2024:\n",
    "                continue\n",
    "\n",
    "            record_key = str(uuid.uuid4())\n",
    "            record_value = line.to_dict()\n",
    "            producer.produce(topic, key=record_key, value=json.dumps(record_value))\n",
    "            producer.poll(0)\n",
    "\n",
    "            print(f\"Produced {total_count} records\")\n",
    "            print(f\"Date: {line['issue_date']}\")\n",
    "            total_count += 1\n",
    "    del df\n",
    "\n",
    "producer.flush()\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
