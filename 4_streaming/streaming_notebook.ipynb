{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Perform the data analysis in a “streaming” manner (treat data as streaming). \n",
    "\n",
    "Use Kafka as a message broker and write a custom producer (reading data from files) and a custom consumer\n",
    "(for processing data). \n",
    "\n",
    "Since data is (with few exceptions, most likely errors) ordered by the “summons number” you can assume that the lines in CSV files are in chronological order\n",
    "(important for producing Kafka messages). \n",
    "\n",
    "Decide if using different topics can help you. \n",
    "\n",
    "Show rolling descriptive statistics (mean, standard deviation, …, think of at least three more) for all data, boroughs, and for the 10 most interesting streets (with highest numbers of tickets overall, or by your qualified choice). \n",
    "\n",
    "For the same data, choose, implement, and apply a stream clustering algorithm (preferably spatial clustering) of your choice [7].\n",
    "\n",
    "Note: stream processing is better not performed in the Arnes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, uuid, json\n",
    "import pandas as pd\n",
    "import faust, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../datasets/'\n",
    "\n",
    "csv_files = [f for f in os.listdir(base_path + \"original_data/\") if f.endswith('.csv')]\n",
    "csv_files.sort()\n",
    "\n",
    "street_locations = pd.read_csv(base_path + 'street_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk):\n",
    "    chunk = chunk.dropna(subset=['summons_number'])\n",
    "    chunk = chunk.drop_duplicates()\n",
    "\n",
    "    chunk['street_code'] = chunk['street_code1'].where(chunk['street_code1'] != 0, chunk['street_code2'].where(chunk['street_code2'] != 0, chunk['street_code3'])).astype(\"string\")\n",
    "    chunk['street_code'] = chunk['street_code'].replace('0', pd.NA)\n",
    "    chunk.drop(['street_code1', 'street_code2', 'street_code3'], axis=1, inplace=True)\n",
    "    chunk.dropna(subset=['street_code'], inplace=True)\n",
    "    chunk['street_code'] = chunk['street_code'].astype(int)\n",
    "    \n",
    "    chunk['issue_date'] = pd.to_datetime(chunk['issue_date'], format=\"mixed\")\n",
    "    \n",
    "    chunk['violation_time'] = chunk['violation_time'].astype(str)\n",
    "    # add M to the end of the time to read it from 12 hour format\n",
    "    chunk['violation_time'] = chunk[\"violation_time\"].str.upper() + \"M\"\n",
    "    # replace the values that starts with 00 to 12\n",
    "    chunk['violation_time'] = chunk['violation_time'].str.replace(r'^00', '12', regex=True)\n",
    "    # convert the time to 24 hour format\n",
    "    chunk['violation_time'] = pd.to_datetime(chunk[\"violation_time\"], format=\"%I%M%p\", errors=\"coerce\")\n",
    "    # combine the date and time\n",
    "    chunk['issue_date'] = chunk[\"issue_date\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"violation_time\"].dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    chunk = chunk.dropna(subset=['issue_date'])\n",
    "    chunk = chunk.drop([\"violation_time\"], axis=1)\n",
    "\n",
    "    #chunk['vehicle_expiration_date'] = pd.to_datetime(chunk['vehicle_expiration_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].astype(str)\n",
    "    chunk['time_first_observed'] = chunk[\"time_first_observed\"].str.upper() + \"M\"\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].str.replace(r'^00', '12', regex=True)\n",
    "    chunk['time_first_observed'] = pd.to_datetime(chunk['time_first_observed'], format='%I%M%p', errors='coerce')\n",
    "\n",
    "    # replace 0 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0', pd.NaT)\n",
    "    # replace 0001-01-03T12:00:00.000 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0001-01-03T12:00:00.000', pd.NaT)\n",
    "\n",
    "    chunk['date_first_observed'] = pd.to_datetime(chunk['date_first_observed'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # merge the date and time\n",
    "    chunk['date_first_observed'] = chunk[\"date_first_observed\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"time_first_observed\"].dt.strftime('%H:%M:%S')\n",
    "    chunk = chunk.drop([\"time_first_observed\"], axis=1)\n",
    "\n",
    "    # translate the county names to the borough names\n",
    "    county_to_borough = {\n",
    "        \"BRONX\": \"BX\", # Bronx\n",
    "        \"BX\": \"BX\",\n",
    "        \"Bronx\": \"BX\",\n",
    "        \"BRONX\": \"BX\",\n",
    "        \"BK\": \"K\", # Brooklyn known as Kings\n",
    "        \"K\": \"K\",\n",
    "        \"Kings\": \"K\",\n",
    "        \"KINGS\": \"K\",\n",
    "        \"KING\": \"K\",\n",
    "        \"Q\": \"Q\", # Queens\n",
    "        \"QN\": \"Q\",\n",
    "        \"Qns\": \"Q\",\n",
    "        \"QUEEN\": \"Q\",\n",
    "        \"QUEENS\": \"Q\",\n",
    "        \"QNS\": \"Q\",\n",
    "        \"QU\": \"Q\",\n",
    "        \"NY\": \"NY\", # Manhattan known as New York\n",
    "        \"MN\": \"NY\",\n",
    "        \"MAN\": \"NY\",\n",
    "        \"NEW Y\": \"NY\",\n",
    "        \"NEWY\": \"NY\",\n",
    "        \"NYC\": \"NY\",\n",
    "        \"ST\": \"R\", # Staten Island known as Richmond\n",
    "        \"R\": \"R\",\n",
    "        \"Rich\": \"R\",\n",
    "        \"RICH\": \"R\",\n",
    "        \"RICHM\": \"R\",\n",
    "        \"RC\": \"R\",\n",
    "        \"MH\": \"NY\",\n",
    "        \"MS\": \"NY\",\n",
    "        \"N\": \"NY\",\n",
    "        \"P\": \"NY\",\n",
    "        \"PBX\": \"NY\",\n",
    "        \"USA\": \"NY\",\n",
    "        \"VINIS\": \"NY\",\n",
    "        \"A\": pd.NA,\n",
    "        \"F\": pd.NA,\n",
    "        \"ABX\": pd.NA,\n",
    "        \"108\": pd.NA,\n",
    "        \"103\": \"R\", # Staten Island zip code\n",
    "        \"00000\": pd.NA,\n",
    "        \"K   F\": pd.NA,\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(county_to_borough)\n",
    "\n",
    "    borough_to_code = {\n",
    "    'NY': 1,\n",
    "    'BX': 2,\n",
    "    'K': 3,\n",
    "    'Q': 4,\n",
    "    'R': 5\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(borough_to_code)\n",
    "\n",
    "    # drop the rows that have NaN in the violation_county\n",
    "    chunk = chunk.dropna(subset=['violation_county'])\n",
    "    \n",
    "    # merge on street_code and StreetCode\n",
    "\n",
    "    chunk = chunk.merge(street_locations, how='left', left_on=['street_code', 'violation_county'], right_on=['StreetCode', 'Borough'])\n",
    "    chunk = chunk.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "    # rename latitude and longitude and drop Street, Borough\n",
    "    chunk = chunk.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
    "    chunk = chunk.drop(['Street'], axis=1)\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = kafka.Producer({'bootstrap.servers': \"localhost:29092\",\n",
    "                  'client.id': socket.gethostname()})\n",
    "                  \n",
    "consumer = kafka.Consumer({'bootstrap.servers': \"localhost:29092\",\n",
    "                           'client.id': socket.gethostname(),\n",
    "                           'group.id': 'test_group', \n",
    "                           'auto.offset.reset': 'earliest'})\n",
    "\n",
    "topic = \"parking_violations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parking_violations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parking_violations.py\n",
    "\n",
    "import faust\n",
    "\n",
    "class ParkingViolation(faust.Record, validation=True):\n",
    "    summons_number: int\n",
    "    plate_id: str\n",
    "    registration_state: str\n",
    "    plate_type: str\n",
    "    violation_code: str\n",
    "    vehicle_body_type: str\n",
    "    vehicle_make: str\n",
    "    issuing_agency: str\n",
    "    street_code: str\n",
    "    vehicle_expiration_date: str\n",
    "    violation_location: str\n",
    "    violation_precinct: str\n",
    "    issuer_precinct: str\n",
    "    issuer_code: str\n",
    "    issuer_command: str\n",
    "    issuer_squad: str\n",
    "    violation_county: str\n",
    "    violation_in_front_of_or_opposite: str\n",
    "    house_number: str\n",
    "    street_name: str\n",
    "    intersecting_street: str\n",
    "    date_first_observed: str\n",
    "    law_section: str\n",
    "    sub_division: str\n",
    "    violation_legal_code: str\n",
    "    days_parking_in_effect: str\n",
    "    from_hours_in_effect: str\n",
    "    to_hours_in_effect: str\n",
    "    vehicle_color: str\n",
    "    unregistered_vehicle: str\n",
    "    vehicle_year: str\n",
    "    meter_number: str\n",
    "    feet_from_curb: str\n",
    "    violation_post_code: str\n",
    "    violation_description: str\n",
    "    no_standing_or_stopping_violation: str\n",
    "    hydrant_violation: str\n",
    "    double_parking_violation: str\n",
    "    latitude: str\n",
    "    longitude: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream_stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_stats.py\n",
    "\n",
    "class StreamStats():\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def update(self, value):\n",
    "        self.count += 1\n",
    "\n",
    "        self.sum += value\n",
    "        self.squared_sum += value ** 2\n",
    "        self.mean = self.sum / self.count\n",
    "\n",
    "        self.min = min(self.min, value)\n",
    "        self.max = max(self.max, value)\n",
    "\n",
    "        self.std = (self.squared_sum / self.count - self.mean ** 2) ** 0.5\n",
    "\n",
    "    def clear(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"mean: {self.mean} +/- {self.std}, min: {self.min}, max: {self.max}\"\n",
    "\n",
    "# Update the stats for each year, month, day, and hour\n",
    "class StreamDateStats():\n",
    "    def __init__(self):\n",
    "        self.year = StreamStats()\n",
    "        self.month = StreamStats()\n",
    "        self.day = StreamStats()\n",
    "\n",
    "        self.year_count = 0\n",
    "        self.month_count = 0\n",
    "        self.day_count = 0\n",
    "\n",
    "        self.total_count = 0\n",
    "\n",
    "    def increase(self):\n",
    "        self.year_count += 1\n",
    "        self.month_count += 1\n",
    "        self.day_count += 1\n",
    "\n",
    "        self.total_count += 1\n",
    "\n",
    "    def reset_year(self):\n",
    "        self.year.update(self.year_count)\n",
    "        self.year_count = 0\n",
    "\n",
    "    def reset_month(self):\n",
    "        self.month.update(self.month_count)\n",
    "        self.month_count = 0\n",
    "\n",
    "    def reset_day(self):\n",
    "        self.day.update(self.day_count)\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_day(self):\n",
    "        self.day.clear()\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_month(self):\n",
    "        self.month.clear()\n",
    "        self.month_count = 0\n",
    "        \n",
    "class CounterDateStats():\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "\n",
    "    def increase(self, key):\n",
    "        if key in [None, \"None\", \"nan\"]:\n",
    "            return\n",
    "\n",
    "        if key not in self.counter:\n",
    "            self.counter[key] = StreamDateStats()\n",
    "\n",
    "        self.counter[key].increase()\n",
    "\n",
    "    def items(self):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)\n",
    "\n",
    "    def most_commons(self, n):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)[:n]\n",
    "\n",
    "    def reset_year(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_year()\n",
    "\n",
    "    def reset_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_month()\n",
    "\n",
    "    def reset_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_day()\n",
    "\n",
    "    def clear_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_month()\n",
    "\n",
    "    def clear_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting faust_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile faust_app.py\n",
    "\n",
    "from typing import List\n",
    "import faust\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "from parking_violations import ParkingViolation\n",
    "from stream_stats import StreamStats, StreamDateStats, CounterDateStats\n",
    "from collections import Counter, deque\n",
    "from river.cluster import DenStream\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Initialize lists to store data points and their cluster IDs\n",
    "buffer_size = 100000\n",
    "data_points = deque(maxlen=buffer_size)\n",
    "cluster_ids = deque(maxlen=buffer_size)\n",
    "\n",
    "topic = \"parking_violations\"\n",
    "\n",
    "app = faust.App(topic, broker='kafka://localhost:29092')\n",
    "print(f\"App is {app}\")\n",
    "\n",
    "violations_topic = app.topic(topic, value_type=ParkingViolation)\n",
    "\n",
    "@app.agent(violations_topic)\n",
    "async def process_violations(violations: List[ParkingViolation]):\n",
    "    # Initialize the DenStream\n",
    "    denstream = DenStream(epsilon=0.01, mu=30, beta=0.2)\n",
    "\n",
    "    all_data_stats = StreamDateStats()\n",
    "    borough_stats = CounterDateStats()\n",
    "    streets_stats = CounterDateStats()\n",
    "\n",
    "    tmp_all_data_stats = StreamDateStats()\n",
    "    tmp_borough_stats = CounterDateStats()\n",
    "    tmp_streets_stats = CounterDateStats()\n",
    "\n",
    "    current_year = None\n",
    "    current_month = None\n",
    "    current_day = None\n",
    "\n",
    "    code_to_borough = {\n",
    "        1: \"Manhattan\",\n",
    "        2: \"Bronx\",\n",
    "        3: \"Brooklyn\",\n",
    "        4: \"Queens\",\n",
    "        5: \"Staten Island\"\n",
    "    }\n",
    "\n",
    "    async for violation in violations:\n",
    "        violation.issue_date = datetime.strptime(violation.issue_date, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        if current_year is None:\n",
    "            current_year = violation.issue_date.year\n",
    "            current_month = violation.issue_date.month\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new day starts, reset the day\n",
    "        if violation.issue_date.day != current_day:\n",
    "            tmp_all_data_stats.reset_day()\n",
    "            tmp_borough_stats.reset_day()\n",
    "            tmp_streets_stats.reset_day()\n",
    "\n",
    "            all_data_stats.reset_day()\n",
    "            borough_stats.reset_day()\n",
    "            streets_stats.reset_day()\n",
    "\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new month starts, print the daily stats of the previous month (mean of violations per day) and clear the stats\n",
    "        if violation.issue_date.month != current_month:\n",
    "            tmp_all_data_stats.reset_month()\n",
    "            tmp_borough_stats.reset_month()\n",
    "            tmp_streets_stats.reset_month()\n",
    "\n",
    "            all_data_stats.reset_month()\n",
    "            borough_stats.reset_month()\n",
    "            streets_stats.reset_month()\n",
    "\n",
    "            print(f\"Daily stats of month {current_month} of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                borough_name = code_to_borough[int(float(borough))]\n",
    "                print(f\".      {borough_name} stats: total: {stats.total_count}, {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_day()\n",
    "            tmp_borough_stats.clear_day()\n",
    "            tmp_streets_stats.clear_day()\n",
    "\n",
    "            current_month = violation.issue_date.month\n",
    "\n",
    "        # if a new year starts, print the stats of the previous year (mean of violations per month) and clear the stats\n",
    "        if violation.issue_date.year != current_year:\n",
    "            tmp_all_data_stats.reset_year()\n",
    "            tmp_borough_stats.reset_year()\n",
    "            tmp_streets_stats.reset_year()\n",
    "\n",
    "            all_data_stats.reset_year()\n",
    "            borough_stats.reset_year()\n",
    "            streets_stats.reset_year()\n",
    "\n",
    "            print(f\"Monthly stats of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.month}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                borough_name = code_to_borough[int(float(borough))]\n",
    "                print(f\".      {borough_name} stats: total: {stats.total_count}, {stats.month}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].month}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_month()\n",
    "            tmp_borough_stats.clear_month()\n",
    "            tmp_streets_stats.clear_month()\n",
    "\n",
    "            current_year = violation.issue_date.year\n",
    "\n",
    "        # print the overall stats of the data every 2000000 records\n",
    "        if all_data_stats.total_count % 2000000 == 0 and all_data_stats.total_count != 0:\n",
    "            # print overall data stats\n",
    "            print(f\"Overall stats from beginning:\")\n",
    "            print(\"- Overall stats: total: {all_data_stats.total_count}\")\n",
    "            print(f\".      yearly: {all_data_stats.year}\")\n",
    "            print(f\".      monthly: {all_data_stats.month}\")\n",
    "            print(f\".      daily: {all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                print(f\".      {borough} stats: {stats.total_count}\")\n",
    "                print(f\".             yearly: {stats.year}\")\n",
    "                print(f\".             monthly: {stats.month}\")\n",
    "                print(f\".             daily: {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}\")\n",
    "                print(f\".             yearly: {street[1].year}\")\n",
    "                print(f\".             monthly: {street[1].month}\")\n",
    "                print(f\".             daily: {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "        # increase the stats with current violation\n",
    "        all_data_stats.increase()\n",
    "        borough_stats.increase(violation.violation_county)\n",
    "        streets_stats.increase(violation.street_code)\n",
    "\n",
    "        tmp_all_data_stats.increase()\n",
    "        tmp_borough_stats.increase(violation.violation_county)\n",
    "        tmp_streets_stats.increase(violation.street_code)\n",
    "\n",
    "        # update the denstream with the current violation\n",
    "        denstream.learn_one({\"latitude\": float(violation.latitude), \"longitude\": float(violation.longitude)})\n",
    "\n",
    "        # Store the data point and its cluster ID\n",
    "        data_points.append((float(violation.latitude), float(violation.longitude)))\n",
    "        cluster_ids.append(denstream.predict_one({\"latitude\": float(violation.latitude), \"longitude\": float(violation.longitude)}))\n",
    "\n",
    "        if all_data_stats.total_count % 100000 == 0 and all_data_stats.total_count != 0:\n",
    "            print(f\"Denstream stats: {denstream.n_clusters}\")\n",
    "\n",
    "            if denstream.n_clusters > 1:\n",
    "                # Plot the clusters\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(\n",
    "                    [point[0] for point in data_points],\n",
    "                    [point[1] for point in data_points],\n",
    "                    c=cluster_ids,\n",
    "                    cmap='viridis',\n",
    "                    marker='o',\n",
    "                    alpha=0.6\n",
    "                )\n",
    "                plt.title('DenStream Clusters')\n",
    "                plt.xlabel('Latitude')\n",
    "                plt.ylabel('Longitude')\n",
    "                plt.colorbar(label='Cluster ID')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in chronological ordered only by issue_date, not by violation_time. So no stats on the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 6760 records out of 10000 (67.60%)\n",
      "Sending 14511 records out of 20000 (72.56%)\n",
      "Sending 20893 records out of 30000 (69.64%)\n",
      "Sending 25170 records out of 40000 (62.92%)\n",
      "Sending 30523 records out of 50000 (61.05%)\n",
      "Sending 36071 records out of 60000 (60.12%)\n",
      "Sending 41226 records out of 70000 (58.89%)\n",
      "Sending 45815 records out of 80000 (57.27%)\n",
      "Sending 54185 records out of 90000 (60.21%)\n",
      "Sending 61959 records out of 100000 (61.96%)\n",
      "Sending 71525 records out of 110000 (65.02%)\n",
      "Sending 79402 records out of 120000 (66.17%)\n",
      "Sending 88862 records out of 130000 (68.36%)\n",
      "Sending 96430 records out of 140000 (68.88%)\n",
      "Sending 105111 records out of 150000 (70.07%)\n",
      "Sending 113720 records out of 160000 (71.08%)\n",
      "Sending 120736 records out of 170000 (71.02%)\n",
      "Sending 130314 records out of 180000 (72.40%)\n",
      "Sending 137878 records out of 190000 (72.57%)\n",
      "Sending 146358 records out of 200000 (73.18%)\n",
      "Sending 154536 records out of 210000 (73.59%)\n",
      "Sending 162573 records out of 220000 (73.90%)\n",
      "Sending 172192 records out of 230000 (74.87%)\n",
      "Sending 179880 records out of 240000 (74.95%)\n",
      "Sending 189437 records out of 250000 (75.77%)\n",
      "Sending 197405 records out of 260000 (75.92%)\n",
      "Sending 206035 records out of 270000 (76.31%)\n",
      "Sending 212633 records out of 280000 (75.94%)\n",
      "Sending 220624 records out of 290000 (76.08%)\n",
      "Sending 228640 records out of 300000 (76.21%)\n",
      "Sending 237733 records out of 310000 (76.69%)\n",
      "Sending 245653 records out of 320000 (76.77%)\n",
      "Sending 255345 records out of 330000 (77.38%)\n",
      "Sending 262971 records out of 340000 (77.34%)\n",
      "Sent: 261758\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m csv_files[\u001b[38;5;241m0\u001b[39m], chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpreprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1456\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/numpy/_core/multiarray.py:1140\u001b[0m, in \u001b[0;36mputmask\u001b[0;34m(a, mask, values)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "send_count = 0\n",
    "current_count = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(base_path + \"original_data/\" + csv_files[0], chunksize=10000)\n",
    "    for chunk in df:\n",
    "        total_count += len(chunk)\n",
    "        chunk = preprocess_chunk(chunk)\n",
    "        send_count += len(chunk)\n",
    "        print(f\"Sending {send_count} records out of {total_count} ({send_count / total_count * 100:.2f}%)\")\n",
    "        for index, line in chunk.iterrows():\n",
    "            year = pd.to_datetime(line['issue_date']).year\n",
    "            if year < 2013 or year > 2024:\n",
    "                continue\n",
    "\n",
    "            record_key = str(uuid.uuid4())\n",
    "            record_value = line.to_dict()\n",
    "            producer.produce(topic, key=record_key, value=json.dumps(record_value))\n",
    "            producer.poll(0)\n",
    "            current_count += 1\n",
    "            # update printing\n",
    "            print(f\"Sent: {current_count}\", end=\"\\r\")\n",
    "        \n",
    "    del df\n",
    "\n",
    "producer.flush()\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
